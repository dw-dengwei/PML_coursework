{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from os import path\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define configurations\n",
    "Set `TEST` to `True` to test the model behavior.\n",
    "Make sure checkpoint is avaliable.\n",
    "Reuse validation variables in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "out_dim = 8\n",
    "num_features = 8\n",
    "use_apex = True # If you haven't install apex, turn false, see more details on \n",
    "                # https://github.com/NVIDIA/apex\n",
    "\n",
    "# environment\n",
    "device = 'cuda' # if don't have GPU, turn 'cpu'\n",
    "random_seed = 0\n",
    "\n",
    "# dataset\n",
    "label_csv_path = '/home/dw-dengwei/dataset/tissue/train.csv'\n",
    "image_root = '/home/dw-dengwei/dataset/tissue/train/'\n",
    "\n",
    "# experiment\n",
    "train_batch_size = 128\n",
    "valid_batch_size = 128\n",
    "epoch_num = 50\n",
    "learning_rate = 0.025336627337952815\n",
    "weight_decay =  0.0046684446208345\n",
    "training_ratio = 0.8\n",
    "save_path = \"save/checkpoint.pth\"\n",
    "TEST = False\n",
    "\n",
    "# SGD optimizer\n",
    "momentum =  0.8\n",
    "nesterov = True\n",
    "\n",
    "# scheduler\n",
    "patience = 5\n",
    "factor = 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# load label and id into memory\n",
    "label_dataframe = pd.read_csv(label_csv_path)\n",
    "label_db = {}\n",
    "ids = []\n",
    "for index, rows in label_dataframe.iterrows():\n",
    "    idx, label = rows['Id'], rows['Cell type']\n",
    "    label_db[str(idx)] = label\n",
    "    ids.append(str(idx))\n",
    "\n",
    "# split training set and validation set\n",
    "n = len(ids)\n",
    "train_ids = random.sample(ids, k=int(n * training_ratio))\n",
    "valid_ids = list(set(ids) - set(train_ids))\n",
    "\n",
    "# import apex\n",
    "if use_apex:\n",
    "    from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define essential classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Cell(Dataset):\n",
    "    def __init__(self, \n",
    "                 ids: list, \n",
    "                 label_db: dict, \n",
    "                 image_root: str, \n",
    "        ) -> None:\n",
    "        '''\n",
    "        ids: indexes of the training/validating data\n",
    "        label_db: dict structure of train.csv file\n",
    "        image_root: image root path\n",
    "        '''\n",
    "\n",
    "        self.num = len(ids)\n",
    "        self.label = []\n",
    "        self.image = []\n",
    "\n",
    "        # These operations may cause large memory in runtime and time use at start.\n",
    "        # Using database file is recommended.\n",
    "        for idx in ids:\n",
    "            target = label_db[idx]\n",
    "            img_cv = cv2.imread(path.join(image_root, str(idx).rjust(6, '0') + \".jpg\"))\n",
    "            img = self.transform(img_cv)\n",
    "            self.label.append(target)\n",
    "            self.image.append(img)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.image[index], self.label[index]\n",
    "\n",
    "    def transform(self, img_cv) -> torch.Tensor:\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "        tf = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x[0:1, :, :]),\n",
    "            ])\n",
    "        return tf(img_pil)\n",
    "\n",
    "\n",
    "# model structure\n",
    "class Toy(nn.Module):\n",
    "    def __init__(self, num_feature, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_feature=num_feature\n",
    "        self.layer = nn.Sequential(\n",
    "            self.make_layer(1, self.num_feature, is_pooling=False),\n",
    "            self.make_layer(self.num_feature, self.num_feature, is_pooling=False),\n",
    "            self.make_layer(self.num_feature, self.num_feature, is_pooling=False),\n",
    "\n",
    "            self.make_layer(self.num_feature, self.num_feature * 2),\n",
    "            self.make_layer(self.num_feature * 2, self.num_feature * 2, is_pooling=False),\n",
    "\n",
    "            self.make_layer(self.num_feature * 2, self.num_feature * 4),\n",
    "\n",
    "            self.make_layer(self.num_feature * 4, self.num_feature * 8),\n",
    "\n",
    "            self.make_layer(self.num_feature * 8, self.num_feature * 16, is_pooling=False),\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(self.num_feature * 16 * 3 * 3, self.num_feature * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.num_feature * 16, num_classes)\n",
    "        )    \n",
    "                \n",
    "        \n",
    "    def make_layer(self, \n",
    "                   in_channels, \n",
    "                   out_channels,\n",
    "                   is_pooling=True,\n",
    "                   kernel_size=3, \n",
    "                   stride=1, \n",
    "                   padding=1):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if is_pooling:\n",
    "            layers.add_module('pooling', nn.AvgPool2d(2, 2))\n",
    "        return layers\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and valitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, epoch: int, dataloader: DataLoader, optimizer, device):\n",
    "    model.train()\n",
    "    pred_true = 0\n",
    "    pred_tot = 0\n",
    "    loss_sum = 0\n",
    "    verbose = 10\n",
    "\n",
    "    batch_acc = []\n",
    "    batch_loss = []\n",
    "\n",
    "    # train batch\n",
    "    for iter, batch in enumerate(dataloader):\n",
    "        image, label = batch\n",
    "        image = image.to(torch.device(device))\n",
    "        label = label.to(torch.device(device))\n",
    "        predict = model.forward(image)\n",
    "        loss = F.cross_entropy(predict, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if use_apex:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred_class = predict.max(1)\n",
    "        pred_true += (label == pred_class).sum()\n",
    "        pred_tot += label.size(0)\n",
    "        loss_sum += loss.item()\n",
    "        acc = (label == pred_class).sum() / label.size(0)\n",
    "        batch_acc.append(acc)\n",
    "        batch_loss.append(loss)\n",
    "        if (iter + 1) % verbose == 0:\n",
    "            print(\"[Train:{}-{}/{}]\\tAcc:{}\\tLoss:{}\".format( \n",
    "                  epoch, \n",
    "                  iter, \n",
    "                  len(dataloader), \n",
    "                  acc, \n",
    "                  loss))\n",
    "\n",
    "    epoch_acc = pred_true / pred_tot\n",
    "    epoch_loss = loss_sum / len(dataloader)\n",
    "    return epoch_acc, batch_acc, epoch_loss, batch_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, epoch: int, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "    pred_true = 0\n",
    "    pred_tot = 0\n",
    "    loss_sum = 0\n",
    "    verbose = 10\n",
    "\n",
    "    batch_acc = []\n",
    "    batch_loss = []\n",
    "\n",
    "    for iter, batch in enumerate(dataloader):\n",
    "        image, label = batch\n",
    "        image = image.to(torch.device(device))\n",
    "        label = label.to(torch.device(device))\n",
    "        predict = model.forward(image)\n",
    "\n",
    "        loss = F.cross_entropy(predict, label)\n",
    "\n",
    "        _, pred_class = predict.max(1)\n",
    "        pred_true += (label == pred_class).sum()\n",
    "        pred_tot += label.size(0)\n",
    "        loss_sum += loss.item()\n",
    "        acc = (label == pred_class).sum() / label.size(0)\n",
    "        batch_acc.append(acc)\n",
    "        batch_loss.append(loss)\n",
    "        if (iter + 1) % verbose == 0:\n",
    "            print(\"[Valid:{}-{}/{}]\\tAcc:{}\\tLoss:{}\".format( \n",
    "                  epoch, \n",
    "                  iter, \n",
    "                  len(dataloader), \n",
    "                  acc, \n",
    "                  loss))\n",
    "\n",
    "\n",
    "    epoch_acc = pred_true / pred_tot\n",
    "    epoch_loss = loss_sum / len(dataloader)\n",
    "    return epoch_acc, batch_acc, epoch_loss, batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "print('dataset and dataloader')\n",
    "dataset_train, dataset_valid = \\\n",
    "    Cell(train_ids, label_db, image_root), \\\n",
    "    Cell(valid_ids, label_db, image_root,)\n",
    "dataloader_train, dataloader_valid = \\\n",
    "    DataLoader(dataset_train, train_batch_size, shuffle=True), \\\n",
    "    DataLoader(dataset_valid, valid_batch_size, shuffle=True), \n",
    "# model\n",
    "print('model')\n",
    "model = Toy(num_feature=num_features, num_classes=out_dim)\n",
    "# load checkpoint before test\n",
    "if TEST:\n",
    "    print('load checkpoint')\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "model = model.to(device=device)\n",
    "\n",
    "# using SGD optimizer\n",
    "print('using SGB optimizer')\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    momentum=momentum,\n",
    "    nesterov=nesterov\n",
    ")\n",
    "# use apex mix precision training\n",
    "if use_apex:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1') \n",
    "\n",
    "# reduce learning rate on plateau\n",
    "print('reduce learning rate on plateau')\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=patience, factor=factor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "print(\"\\033[0;31;40mStart Training\\033[0m\")\n",
    "train_loss_per_batch = []\n",
    "train_loss_per_epoch = []\n",
    "train_acc_per_batch = []\n",
    "train_acc_per_epoch = []\n",
    "\n",
    "valid_loss_per_batch = []\n",
    "valid_loss_per_epoch = []\n",
    "valid_acc_per_batch = []\n",
    "valid_acc_per_epoch = []\n",
    "if not TEST:\n",
    "    for epoch in range(epoch_num):\n",
    "        train_acc, train_batch_acc, train_loss, train_batch_loss = \\\n",
    "            train_epoch(model, epoch, dataloader_train, optimizer, device)\n",
    "        valid_acc, valid_batch_acc, valid_loss, valid_batch_loss = \\\n",
    "            evaluate(model, epoch, dataloader_valid, device)\n",
    "\n",
    "        train_loss_per_batch.append({epoch: train_batch_loss})\n",
    "        train_loss_per_epoch.append(train_loss)\n",
    "        train_acc_per_batch.append({epoch: train_batch_acc})\n",
    "        train_acc_per_epoch.append(train_acc)\n",
    "\n",
    "        valid_loss_per_batch.append({epoch: valid_batch_loss})\n",
    "        valid_loss_per_epoch.append(valid_loss)\n",
    "        valid_acc_per_batch.append({epoch: valid_batch_acc})\n",
    "        valid_acc_per_epoch.append(valid_acc)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        print(\"[Train:{}]\\tAcc:{}\\tLoss:{}\"\\\n",
    "            .format(epoch, train_acc, train_loss))\n",
    "        print(\"\\033[0;31;40m[Valid:{}]\\tAcc:{}\\tLoss:{}\\033[0m\"\\\n",
    "            .format(epoch, valid_acc, valid_loss))\n",
    "else:\n",
    "    valid_acc, valid_batch_acc, valid_loss, valid_batch_loss = \\\n",
    "        evaluate(model, dataloader_valid, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST:\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump experiment logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "\n",
    "save_variable(train_acc_per_epoch,  'save/train_acc_per_epoch.bin')\n",
    "save_variable(train_loss_per_epoch, 'save/train_loss_per_epoch.bin')\n",
    "\n",
    "save_variable(valid_acc_per_epoch,  'save/valid_acc_per_epoch.bin')\n",
    "save_variable(valid_loss_per_epoch, 'save/valid_loss_per_epoch.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "train_loss_per_epoch = load_variavle('save/train_loss_per_epoch.bin')\n",
    "valid_loss_per_epoch = load_variavle('save/valid_loss_per_epoch.bin')\n",
    "\n",
    "train_acc_per_epoch  = load_variavle('save/train_acc_per_epoch.bin')\n",
    "valid_acc_per_epoch  = load_variavle('save/valid_acc_per_epoch.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_torch(tensor_list: list) -> list:\n",
    "    ret = tensor_list.copy()\n",
    "    for i, _ in enumerate(ret):\n",
    "        ret[i] = ret[i].item()\n",
    "    \n",
    "    return ret\n",
    "\n",
    "pd_train_loss_per_epoch = pd.Series(train_loss_per_epoch)\n",
    "pd_valid_loss_per_epoch = pd.Series(valid_loss_per_epoch)\n",
    "pd_train_acc_per_epoch  = pd.Series(remove_torch(train_acc_per_epoch))\n",
    "pd_valid_acc_per_epoch  = pd.Series(remove_torch(valid_acc_per_epoch))\n",
    "loss = pd.concat([pd_train_loss_per_epoch, pd_valid_loss_per_epoch], axis=1)\n",
    "acc  = pd.concat([pd_train_acc_per_epoch, pd_valid_acc_per_epoch], axis=1)\n",
    "\n",
    "loss.columns = ['Training', 'Validation']\n",
    "acc.columns = ['Training', 'Validation']\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(acc, label=acc.columns)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"save/acc.png\", dpi=700, format='png')\n",
    "plt.savefig(\"save/acc.svg\", format='svg')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(loss, label=loss.columns)\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"save/loss.png\", dpi=700, format='png')\n",
    "plt.savefig(\"save/loss.svg\", format='svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecd59a109af3e75fb1e8b2befe8d8aff484cbbe1e2075db6bede26970313e655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
